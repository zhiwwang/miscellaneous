{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,random,re\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "from fake_useragent import UserAgent\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    print(u'登陆新浪微博手机端...')\n",
    "    browser = webdriver.Firefox()\n",
    "    ##给定登陆的网址\n",
    "    url = 'https://passport.weibo.cn/signin/login'\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "    #找到输入用户名的地方\n",
    "    username = browser.find_element_by_css_selector('#loginName')\n",
    "    time.sleep(2)\n",
    "    username.clear()\n",
    "    username.send_keys('15528076813')#输入自己的账号\n",
    "    #找到输入密码的地方\n",
    "    password = browser.find_element_by_css_selector('#loginPassword')\n",
    "    time.sleep(2)\n",
    "    password.send_keys('wANG123456')\n",
    "    #点击登录\n",
    "    browser.find_element_by_css_selector('#loginAction').click()\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    print('########出现Error########')\n",
    "finally:\n",
    "    print('完成登陆!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#话题\n",
    "topic='植物肉'\n",
    "\n",
    "#是否只关注话题\n",
    "focus_topic=True\n",
    "\n",
    "#结果页数\n",
    "page_number=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只关注话题\n",
    "if focus_topic:\n",
    "    topic='%23'+topic+'%23'\n",
    "\n",
    "# 获取微博话题下的内容\n",
    "def get_page(i):\n",
    "    url = 'https://s.weibo.com/weibo?q='+topic+'&page='+str(i) \n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "#将微博折叠隐藏的内容展开\n",
    "def show_all():\n",
    "    num = len(browser.find_elements(\"partial link text\",'展开'))   \n",
    "    for i in range(num):\n",
    "        browser.find_element(\"partial link text\",'展开').click()\n",
    "        time.sleep(1)\n",
    "\n",
    "#提取微博文本内容\n",
    "text = []\n",
    "def get_info():\n",
    "    info = browser.find_elements(\"xpath\",\"//p[@class='txt']\")\n",
    "    for value in info: \n",
    "        text.append(value.text)\n",
    "\n",
    "if page_number==1:\n",
    "    url = 'https://s.weibo.com/weibo?q='+topic\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "    show_all()\n",
    "    get_info()\n",
    "else:\n",
    "    for i in tqdm(range(1,page_number)):\n",
    "        get_page(i)\n",
    "        show_all()\n",
    "        get_info()\n",
    "\n",
    "#删除空元素\n",
    "while '' in text:\n",
    "    text.remove('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for test\n",
    "\n",
    "# print(len(text))\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "file1 = './data/'+topic+'.txt'\n",
    "file2 = './data/'+topic+'.csv'\n",
    "\n",
    "with codecs.open(file1,'w+',encoding='utf-8') as f:\n",
    "    f.write(\"\")   \n",
    "with codecs.open(file2,'w+',encoding='utf-8') as f:\n",
    "    f.write(\"\") \n",
    "\n",
    "with open('./data/'+topic+'.txt',\"w\",encoding='utf-8') as f:\n",
    "    for i in range(len(text)):\n",
    "        f.write(text[i])\n",
    "\n",
    "pd.DataFrame(text).to_csv('./data/'+topic+'.csv',encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "filepath_p = './data/'+topic+'.csv'\n",
    "data = pd.read_csv(filepath_p)\n",
    "data.head()\n",
    "\n",
    "theme = data\n",
    "theme=theme.dropna()  ##删除缺失值\n",
    "theme.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊符号去除\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "r1 = \"[^\\u4e00-\\u9fa5]\"\n",
    "\n",
    "theme['clean_text'] = ''\n",
    "text_len = theme.shape[0]\n",
    "for i in range(text_len):\n",
    "    a = re.sub(r1,'',theme.iloc[i,0])\n",
    "    theme.iloc[i,1] = a\n",
    "theme.head()\n",
    "\n",
    "theme.clean_text.to_csv(\"./data/clean_\"+topic+\".txt\",index=False)\n",
    "with open(\"./data/clean_\"+topic+\".txt\",'r',encoding='UTF-8') as f:\n",
    "    contents = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 载入标准停用词库\n",
    "stopwords1= [line.rstrip() for line in open('./stopwords/baidu_stopwords.txt', 'r', encoding='utf-8')]\n",
    "stopwords2= [line.rstrip() for line in open('./stopwords/cn_stopwords.txt', 'r', encoding='utf-8')]\n",
    "stopwords3 = [line.rstrip() for line in open('./stopwords/scu_stopwords.txt', 'r', encoding='utf-8')]\n",
    "stopwords4 = [line.rstrip() for line in open('./stopwords/hit_stopwords.txt', 'r', encoding='utf-8')]\n",
    "\n",
    "\n",
    "#自定义停用词\n",
    "stopwords_mine=[\"收起\",\"全文\",\"视频\",\"微博\"]\n",
    "\n",
    "stopwords = stopwords1 + stopwords2 + stopwords3+stopwords_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗停用词数据\n",
    "file3=\"./data/words_\"+topic+\".txt\"\n",
    "with codecs.open(file3,'w+',encoding='utf-8') as f:\n",
    "    f.write(\"\")   \n",
    "\n",
    "output = open(file3, \"w\", encoding='utf8')\n",
    "\n",
    "# 使用jieba分词，获取词的列表\n",
    "contents_cut = jieba.cut(contents)\n",
    "for word in contents_cut:\n",
    "    if  word not in stopwords:\n",
    "        output.write(word + \" \")\n",
    "\n",
    "contents_list = \" \".join(contents_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "file = open(file3, \"r\", encoding='utf-8') \n",
    "txt = file.read()\n",
    "\n",
    "words = jieba.lcut(txt) # 使用jieba进行分词，将文本分成词语列表\n",
    "count = {}\n",
    "\n",
    "for word in words: \n",
    "    #纳入统计的词长度\n",
    "    if len(word) > 1: \n",
    "        count[word] = count.get(word, 0) + 1 \n",
    "\n",
    "\n",
    "# 删除无关词\n",
    "# 自定义无关词语\n",
    "exclude = [\"可以\", \"一起\", \"这样\",\"clean\",\"text\"] \n",
    "for key in list(count.keys()): \n",
    "    if key in exclude:\n",
    "        del count[key] \n",
    "\n",
    "list_words = list(count.items()) \n",
    "\n",
    "list_words.sort(key=lambda x: x[1], reverse=True) \n",
    "\n",
    "for i in range(5): \n",
    "    word, number = list_words[i]\n",
    "print(\"关键字：{:-<10}频次：{:+>8}\".format(word, number))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e284ee3255a07ad8bf76694974743c4c81cb57e7c969474d752d949b11d721e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
